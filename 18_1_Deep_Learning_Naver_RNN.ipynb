{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKsV7MhOBOfgns6y02UqFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clzndyal1/git_test/blob/master/18_1_Deep_Learning_Naver_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "cqG3-W6THh5b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Embedding, LSTM, Activation, Conv1D, MaxPooling1D\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.datasets import mnist, reuters, imdb\n",
        "from tensorflow.keras.utils import to_categorical  #one-hot encoding과 같은 결과 반환\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "import numpy as np, array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request"
      ],
      "metadata": {
        "id": "T4S6-2GrHo2B"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
        "                           filename=\"./data/ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\",\n",
        "                           filename=\"./data/ratings_test.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeqkSuymHt2R",
        "outputId": "a36c2639-d01d-42f5-c165-1fbe7e8dabf5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./data/ratings_test.txt', <http.client.HTTPMessage at 0x7fde096b8ee0>)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_table('./data/ratings_train.txt')\n",
        "test_data = pd.read_table('./data/ratings_test.txt')"
      ],
      "metadata": {
        "id": "cpeIo44EH786"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "    # 1. 비어 있는 리뷰 처리\n",
        "    # 2. 한글외의 문자는 Nan 처리\n",
        "    # 3. 같은 리뷰 제거\n",
        "\n",
        "# 1. 비어 있는 리뷰 처리\n",
        "train_data = train_data.dropna(how='any')  # null 존재하는 행 제거\n",
        "# 2. 한글을 제외한 문자를 '' 로 변경\n",
        "train_data['document'] = train_data['document'].str.replace(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 \" \"]',\n",
        "                                                          '', regex=True)\n",
        "# '' 를 np.nan  으로 변경 -> nan 을 제거\n",
        "train_data['document'].replace('', np.nan, inplace=True)\n",
        "train_data.dropna(inplace=True)\n",
        "# 3. 같은 리뷰 제거\n",
        "train_data.drop_duplicates(subset=['document'], inplace=True)"
      ],
      "metadata": {
        "id": "5062WurXIamv"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  학습 데이터와 테스트 데이터로 정형화\n",
        "    # 4. x 와 y로 분리\n",
        "    # 5. 단어로 토큰\n",
        "    # 6. 패딩\n",
        "\n",
        "# 4. x 와 y로 분리\n",
        "# Training data\n",
        "x_train = train_data['document']\n",
        "y_train = train_data['label']\n",
        "# Testing data\n",
        "x_test = test_data['document']\n",
        "y_test = test_data['label']"
      ],
      "metadata": {
        "id": "hW2T0CUPUHus"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3awB3KfRI3b-",
        "outputId": "00ec7d34-9fb9-495e-e660-58201b113ed4"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(143691,) (143691,)\n",
            "(50000,) (50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and padding\n",
        "token_train = Tokenizer()\n",
        "token_train.fit_on_texts(x_train)\n",
        "train_sequences = token_train.texts_to_sequences(x_train)\n",
        "test_sequences = token_train.texts_to_sequences(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "i1o35QykU7MN",
        "outputId": "e50a606b-f873-4d9a-da48-266c863819c1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-72502951de54>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtoken_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \"\"\"\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[1;32m    387\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sequence , token,-> token.rowd_index +1"
      ],
      "metadata": {
        "id": "vvZUUynbUZ4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델생성\n",
        "model = Sequential()   #모델 선택\n",
        "model.add(Embedding(143691, 1000))   #단어 임베딩\n",
        "model.add(Conv1D(200, 20))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(Dropout(0.5))        #  과적합 방지를 위한  dropout\n",
        "model.add(LSTM(55))   #노드 수\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "4PH74sWDIcwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(loss ='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "hist = model.fit(x_train, y_train, epochs=200, batch_size=40, validation_split=0.25, callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "ajzV8AVyIeuH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}